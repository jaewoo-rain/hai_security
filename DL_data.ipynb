{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9417bed6",
   "metadata": {},
   "source": [
    "센서 컬럼 자동 탐색\n",
    "통계 기반 이상치(outlier) 플래그\n",
    "Z-score 정규화\n",
    "슬라이딩 윈도우 + 라벨\n",
    "시계열 분할\n",
    "WeightedRandomSampler 기반 배치 불균형 보정\n",
    "LSTM 분류기 학습\n",
    "PR-Curve 기반 임계치 결정 → 최종 평가\n",
    "\n",
    "하이퍼파라미터(W, S, hidden_dim, batch_size 등)만 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29b15170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  Loss: 0.082406\n",
      "Epoch 02  Loss: 0.007162\n",
      "Epoch 03  Loss: 0.018146\n",
      "Epoch 04  Loss: 0.013875\n",
      "Epoch 05  Loss: 0.012732\n",
      "Epoch 06  Loss: 0.004660\n",
      "Epoch 07  Loss: 0.003619\n",
      "Epoch 08  Loss: 0.000752\n",
      "Epoch 09  Loss: 0.000101\n",
      "Epoch 10  Loss: 0.000051\n",
      "Epoch 11  Loss: 0.000032\n",
      "Epoch 12  Loss: 0.000023\n",
      "Epoch 13  Loss: 0.000018\n",
      "Epoch 14  Loss: 0.000015\n",
      "Epoch 15  Loss: 0.000013\n",
      "Epoch 16  Loss: 0.000010\n",
      "Epoch 17  Loss: 0.000009\n",
      "Epoch 18  Loss: 0.000008\n",
      "Epoch 19  Loss: 0.000007\n",
      "Epoch 20  Loss: 0.000006\n",
      "Best threshold = 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9994    1.0000    0.9997      1743\n",
      "           1     1.0000    0.9333    0.9655        15\n",
      "\n",
      "    accuracy                         0.9994      1758\n",
      "   macro avg     0.9997    0.9667    0.9826      1758\n",
      "weighted avg     0.9994    0.9994    0.9994      1758\n",
      "\n",
      "Test PR-AUC: 0.9958333333333333\n"
     ]
    }
   ],
   "source": [
    "# 전체 파이프라인 코드 (지정 센서 컬럼 + SMOTE 오버샘플링 + RandomUnderSampler)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, average_precision_score\n",
    "from scipy.special import expit\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 1. 데이터 로드 및 정렬\n",
    "df = pd.read_csv('../train2.csv', sep=';', parse_dates=['time'])\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "# 2. 사용할 센서 컬럼 명시\n",
    "sensor_cols = [\n",
    "    'P1_LCV01D', 'P1_B2016', 'P1_PIT01',\n",
    "    'P3_LCV01D', 'P2_SIT01', 'P1_LIT01',\n",
    "    'P1_FT03',   'P1_PCV01D'\n",
    "]\n",
    "\n",
    "# 3. 통계 기반 이상치 플래그 (|z| ≥ 3)\n",
    "zscores = df[sensor_cols].apply(stats.zscore)\n",
    "for col in sensor_cols:\n",
    "    df[f'{col}_outlier'] = (zscores[col].abs() >= 3).astype(float)\n",
    "\n",
    "# 4. Z-score 정규화\n",
    "for col in sensor_cols:\n",
    "    μ, σ = df[col].mean(), df[col].std()\n",
    "    df[f'{col}_norm'] = (df[col] - μ) / σ\n",
    "\n",
    "# 5. 슬라이딩 윈도우 생성 및 노이즈 윈도우 제거\n",
    "W, S = 100, 10\n",
    "feat_cols = [f'{c}_norm'    for c in sensor_cols] + \\\n",
    "            [f'{c}_outlier' for c in sensor_cols]\n",
    "\n",
    "windows, labels = [], []\n",
    "L = len(df)\n",
    "for i in range(0, L - W + 1, S):\n",
    "    win = df.iloc[i:i+W]\n",
    "    # 노이즈 윈도우: attack==0 이면서 any outlier True\n",
    "    if (win['attack'] == 0).all() and win[[f'{c}_outlier' for c in sensor_cols]].any(axis=None):\n",
    "        continue\n",
    "    windows.append(win[feat_cols].values.astype(np.float32))\n",
    "    labels.append(int(win['attack'].any()))\n",
    "\n",
    "X = np.stack(windows)                # (n_windows, W, 2*len(sensor_cols))\n",
    "y = np.array(labels, dtype=np.int64) # 0=정상, 1=실제 이상\n",
    "\n",
    "# 6. Stratified train/val/test 분할 (70/15/15)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=42\n",
    ")\n",
    "val_frac = 0.15 / 0.85\n",
    "X_train_raw, X_val, y_train_raw, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=val_frac, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "# 7. SMOTE + RandomUnderSampler 파이프라인\n",
    "n_train, Wc, Fc = X_train_raw.shape\n",
    "X_flat = X_train_raw.reshape(n_train, Wc * Fc)\n",
    "imb_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(sampling_strategy=0.5, random_state=42)),\n",
    "    ('under', RandomUnderSampler(sampling_strategy=0.6, random_state=42))\n",
    "])\n",
    "X_res, y_res = imb_pipeline.fit_resample(X_flat, y_train_raw)\n",
    "X_train = X_res.reshape(-1, Wc, Fc)\n",
    "y_train = y_res.astype(np.int64)\n",
    "\n",
    "# 8. PyTorch Dataset & DataLoader\n",
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(SensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(SensorDataset(X_val,   y_val),   batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(SensorDataset(X_test,  y_test),  batch_size=64, shuffle=False)\n",
    "\n",
    "# 9. LSTM 분류기 정의\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc   = nn.Linear(hidden_dim*2, 1)\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        return self.fc(h).squeeze(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = LSTMClassifier(len(feat_cols)).to(device)\n",
    "\n",
    "# 10. Loss & Optimizer 설정\n",
    "counts = np.bincount(y_train, minlength=2)\n",
    "pos_weight = torch.tensor([counts[0]/counts[1]], device=device) if counts[1]>0 else torch.tensor([1.0], device=device)\n",
    "criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 11. 학습 루프\n",
    "for epoch in range(1, 21):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.float().to(device)\n",
    "        logits = model(xb)\n",
    "        loss   = criterion(logits, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    print(f'Epoch {epoch:02d}  Loss: {total_loss/len(train_loader.dataset):.6f}')\n",
    "\n",
    "# 12. Validation → Threshold Tuning\n",
    "model.eval()\n",
    "val_logits = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        val_logits.extend(model(xb).cpu().numpy())\n",
    "\n",
    "val_logits = np.array(val_logits)\n",
    "mask = np.isfinite(val_logits)\n",
    "val_probs = expit(val_logits[mask])\n",
    "y_val_filt = y_val[mask]\n",
    "prec, rec, th = precision_recall_curve(y_val_filt, val_probs)\n",
    "f1_scores = 2*prec*rec/(prec+rec+1e-8)\n",
    "best_thresh = th[np.nanargmax(f1_scores)]\n",
    "print(f'Best threshold = {best_thresh:.4f}')\n",
    "\n",
    "# 13. Test 평가\n",
    "test_probs, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb).cpu().numpy()\n",
    "        probs = expit(logits)\n",
    "        test_probs.extend(probs)\n",
    "        y_pred.extend((probs >= best_thresh).astype(int))\n",
    "\n",
    "test_probs = np.array(test_probs)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Test PR-AUC:\", average_precision_score(y_test, test_probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d3f11",
   "metadata": {},
   "source": [
    "추가로 5-fold함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d7fc65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Fold 1 Recall: 1.0000\n",
      "Fold 2 Recall: 1.0000\n",
      "Fold 3 Recall: 0.9375\n",
      "Fold 4 Recall: 1.0000\n",
      "Fold 5 Recall: 0.9412\n",
      "CV Recall: 0.9757 ± 0.0297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9994    0.9997      1743\n",
      "           1     0.9375    1.0000    0.9677        15\n",
      "\n",
      "    accuracy                         0.9994      1758\n",
      "   macro avg     0.9688    0.9997    0.9837      1758\n",
      "weighted avg     0.9995    0.9994    0.9994      1758\n",
      "\n",
      "Saved scalers.pth and model_final.pth\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import recall_score, classification_report\n",
    "from scipy.special import expit\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 1. 데이터 로드 및 정렬\n",
    "df = pd.read_csv('../train2.csv', sep=';', parse_dates=['time'])\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "# 2. 사용할 센서 컬럼 명시\n",
    "sensor_cols = [\n",
    "    'P1_LCV01D','P1_B2016','P1_PIT01',\n",
    "    'P3_LCV01D','P2_SIT01','P1_LIT01',\n",
    "    'P1_FT03','P1_PCV01D'\n",
    "]\n",
    "\n",
    "# 3. 이상치 플래그 및 정규화\n",
    "zscores = df[sensor_cols].apply(stats.zscore)\n",
    "for c in sensor_cols:\n",
    "    df[f'{c}_outlier'] = (zscores[c].abs() >= 3).astype(float)\n",
    "    mu, sigma = df[c].mean(), df[c].std()\n",
    "    df[f'{c}_norm'] = (df[c] - mu) / sigma\n",
    "\n",
    "# 4. 슬라이딩 윈도우 생성 및 노이즈 제거\n",
    "W, S = 100, 10\n",
    "feat_cols = [f'{c}_norm' for c in sensor_cols] + [f'{c}_outlier' for c in sensor_cols]\n",
    "windows, labels = [], []\n",
    "L = len(df)\n",
    "for i in range(0, L - W + 1, S):\n",
    "    win = df.iloc[i:i+W]\n",
    "    if (win['attack'] == 0).all() and win[[f'{c}_outlier' for c in sensor_cols]].any(axis=None):\n",
    "        continue\n",
    "    windows.append(win[feat_cols].values.astype(np.float32))\n",
    "    labels.append(int(win['attack'].any()))\n",
    "\n",
    "X = np.stack(windows)\n",
    "y = np.array(labels, dtype=int)\n",
    "\n",
    "# 5. Test 분리 (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
    "\n",
    "# 6. 5-Fold Stratified CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_recalls = []\n",
    "\n",
    "# LSTM 모델 정의\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h_cat = torch.cat([h[-2], h[-1]], dim=1)\n",
    "        return self.fc(h_cat).squeeze(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_temp, y_temp), 1):\n",
    "    X_tr, y_tr = X_temp[train_idx], y_temp[train_idx]\n",
    "    X_val, y_val = X_temp[val_idx], y_temp[val_idx]\n",
    "\n",
    "    # SMOTE + RandomUnderSampler\n",
    "    n, w, f = X_tr.shape\n",
    "    X_flat = X_tr.reshape(n, w * f)\n",
    "    imb = ImbPipeline([\n",
    "        ('smote', SMOTE(sampling_strategy=0.5, random_state=42)),\n",
    "        ('under', RandomUnderSampler(sampling_strategy=0.6, random_state=42))\n",
    "    ])\n",
    "    X_res, y_res = imb.fit_resample(X_flat, y_tr)\n",
    "    X_train = X_res.reshape(-1, w, f)\n",
    "    y_train = y_res.astype(int)\n",
    "\n",
    "    # Dataset & DataLoader\n",
    "    class SSData(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "        def __len__(self):\n",
    "            return len(self.y)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    train_loader = DataLoader(SSData(X_train, y_train), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(SSData(X_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "    # Model, Loss, Optimizer\n",
    "    model = LSTMClassifier(len(feat_cols)).to(device)\n",
    "    counts = np.bincount(y_train, minlength=2)\n",
    "    pw = counts[0] / counts[1] if counts[1] > 0 else 1.0\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pw], device=device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(1, 11):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.float().to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        #     total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # # 배치 루프가 끝난 후, 여기서 한 번만 출력\n",
    "        # avg_loss = total_loss / len(train_loader.dataset)\n",
    "        # print(f'Epoch {epoch:02d}  Loss: {avg_loss:.6f}')\n",
    "\n",
    "    # Validation (Threshold 0.5)\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            probs = expit(model(xb.to(device)).cpu().numpy())\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            y_true.extend(yb.numpy())\n",
    "            y_pred.extend(preds)\n",
    "    r = recall_score(y_true, y_pred)\n",
    "    cv_recalls.append(r)\n",
    "    print(f'Fold {fold} Recall: {r:.4f}')\n",
    "\n",
    "print(f'CV Recall: {np.mean(cv_recalls):.4f} ± {np.std(cv_recalls):.4f}')\n",
    "\n",
    "# 7. Final Test Evaluation (Threshold 0.5)\n",
    "SS_dataset = SSData(X_test, y_test)\n",
    "test_loader = DataLoader(SS_dataset, batch_size=64, shuffle=False)\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_loader:\n",
    "        probs = expit(model(xb.to(device)).cpu().numpy())\n",
    "        y_pred.extend((probs >= 0.5).astype(int))\n",
    " \n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# 학습 루프 끝난 뒤\n",
    "torch.save(scalers, 'scalers.pth')\n",
    "torch.save(model.state_dict(), 'model_final.pth')\n",
    "print(\"Saved scalers.pth and model_final.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51dc82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  time  anomaly  probability\n",
      "0  2019-10-29 11:01:39        0     0.000130\n",
      "1  2019-10-29 11:01:49        0     0.000143\n",
      "2  2019-10-29 11:01:59        0     0.000230\n",
      "3  2019-10-29 11:02:09        0     0.000119\n",
      "4  2019-10-29 11:02:19        0     0.000115\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.special import expit\n",
    "\n",
    "# 1. 학습 때 사용한 센서+윈도우 파라미터\n",
    "sensor_cols = [\n",
    "    'P1_LCV01D','P1_B2016','P1_PIT01',\n",
    "    'P3_LCV01D','P2_SIT01','P1_LIT01',\n",
    "    'P1_FT03','P1_PCV01D'\n",
    "]\n",
    "W, S = 100, 10\n",
    "feat_cols = [f'{c}_norm' for c in sensor_cols] + [f'{c}_outlier' for c in sensor_cols]\n",
    "THRESH = 0.5  # 이상 판단 임계치\n",
    "\n",
    "# 2. 스케일러와 모델 불러오기\n",
    "scalers = torch.load('scalers.pth', weights_only=False)    # {sensor: (mu, sigma), ...}\n",
    "model = LSTMClassifier(len(feat_cols), hidden_dim=128, num_layers=2)\n",
    "model.load_state_dict(torch.load('model_final.pth'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device).eval()\n",
    "\n",
    "# 3. test1.csv 전처리\n",
    "df = pd.read_csv('../test1.csv', sep=';', parse_dates=['time']) \\\n",
    "       .sort_values('time') \\\n",
    "       .reset_index(drop=True)\n",
    "\n",
    "# 이상치 플래그 & 정규화\n",
    "zscores = df[sensor_cols].apply(stats.zscore)\n",
    "for c in sensor_cols:\n",
    "    df[f'{c}_outlier'] = (zscores[c].abs() >= 3).astype(float)\n",
    "    mu, sigma = scalers[c]\n",
    "    df[f'{c}_norm'] = (df[c] - mu) / sigma\n",
    "\n",
    "# 4. 슬라이딩 윈도우 Dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, feat_cols, W, S):\n",
    "        windows, times = [], []\n",
    "        L = len(df)\n",
    "        for i in range(0, L - W + 1, S):\n",
    "            win = df.iloc[i:i+W]\n",
    "            windows.append(win[feat_cols].values.astype(np.float32))\n",
    "            # Timestamp → ISO string 변환\n",
    "            times.append(win['time'].iloc[-1].isoformat())\n",
    "        self.X = torch.from_numpy(np.stack(windows))  # (num_windows, W, num_features)\n",
    "        self.times = times\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.times)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.times[idx]\n",
    "\n",
    "test_ds = TestDataset(df, feat_cols, W, S)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "# 5. 추론\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for xb, times in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb).cpu().numpy()\n",
    "        probs  = expit(logits).squeeze(-1)   # shape: (batch,)\n",
    "        preds  = (probs >= THRESH).astype(int)\n",
    "        for t, p, pr in zip(times, preds, probs):\n",
    "            results.append({\n",
    "                'time':       t,\n",
    "                'anomaly':    int(p),\n",
    "                'probability': float(pr)\n",
    "            })\n",
    "\n",
    "# 6. 결과 저장\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_csv('test1_predictions.csv', index=False)\n",
    "print(df_out.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
